---
layout: post
title: 02/05/2017 Quiz I Notes
description: Quiz I Notes for Computer Graphics
modified: 2017-02-05
tags: [IR, notes]
---

## Lecture Zero: Intro

  * Why IR
      * Due to information overload
      * Unstructured data

  * Main topics
      * Search Engine Arch
      * Retrieval models
      * Evaluations
      * feedback
      * link analysis
      * Search applications

  * Core Concepts
    * Query representation
      * lexical gap
      * Semantic gap
    * Document representation
      * data structure for access
      * handle lexical + semantic gap
    * Retrieval model
      * based on the query give **most relevant** result back

    * IR vs DB
    * IR vs NLP

## Lecture I: Search Engine Architecture

  * Paper to read: **The Anatomy of a Large-Scale Hypertextual Web Search Engine**

  * Key parts:
    * Crawler and indexer
    * Document Analyzer
    * Query parser
    * Ranking model

  * Flow path:
    * query side:
      * user input -> Query parser -> ranking model -> domain db -> result preprocessing -> result display
    * document side:
      * crawler & indexer -> Document Analyzer & Auxiliary database

  * Another looking:
    * crawler -> Doc Analyzer -> indexer ->
    * Query -> Query Rep ->
    * Combined to -> ranker -> result
    * Receive feedback -> modify ranker & Query Rep

  * Key concepts
    * Information need
    * Query
    * Document
    * Relevance
    * **rank documents by their relevance to user's Information need**
    * Web crawler
    * Document Analyzer & indexer
        * manage crawled content and provide efficient access
    * Query parser
        * compile user input into comp. rep.
    * Ranking model
        * sort the candidate documents by its relevancy to the given query
    * Result Display
        * Present the retrieved results
    * Retrieval Evaluation
        * Assess the quality of the result
    * Relevance feedback
        * propagate the judgment back to the system to improve performance
    * Search Query log
        * record user's interaction with the system
    * User modeling
        * Assess user's satisfaction with the search engine
        * understand user longitude info need
    * Search Style
        * Browsing -- Browse all categories
        * Querying -- Completely based on user query
    * Pull vs Push
        * Pull, with query -- user with initiative pull info out
        * push, without query -- system push info out based on user log

## Lecture II: Web Crawling & Text analysis
  * Psuedo code

  ```
  Def Crawler(entry) {
    Stack = [entry]
    while (!Stack.empty)
      Current_URL = Stack.pop();
      if (visited(Current_URL)) | isIllegal(Current_URL) | checkRobotsText(Current_URL)
      continue
      page = open(Current_URL)
      for (anchor in page)
        Stack.append(anchor)
      setVisited(Current_URL)
      insertToIndex(HTML)
  }
  ```
  * Strategies
    * BFS
      * uniformly browse the web
      * like the pseudo code given
    * DFS
      * Explore the web by branch
      * web is not tree so biased ?
    * Focused Crawling
      * idea
        * prioritize links by given strategy
        * get a higher weighted average
      * prioritize by in-degree
        * Take the page with highest number of hyperlinks from previous web
      * PageRank
        * BFS in early stage then compute PageRank periodically
      * Topical relevance
        * only crawl relevant pages(vertical search)
        * Estimate similarity to current page(by anchortext, or something near)
        * user given taxonomy or topical classifier?
      * Avoid duplicate visit
        * use trie? or hash
        * most normalize url(truncate unnecessary tail) before check
      * Robot exclusion protocol

    * Text analysis
      * HTML parsing
        * shallow parsing
          * remove html tags
          * only take title/p
        * Automatic wrapper generation
          * wrapper generation
            * wrapper: regular expression for tags combination
            * inductive reasoning from examples
        * Visual parsing
          * Frequent pattern mining of similar html block
      * Tokenization
        * break stream of text into meaningful units (could be more than words)
        * Solutions
          * regular expression to parse whitespace and combine 's'
          * Statistical method to explore features of texts (NLP library)

    * Normalization
      * Convert different forms of words into the vocabulary
      * Solution
        * Rule-based: delete periods, lower cases, etc
        * Dictionary-based: construct equivalence class

    * Stemming
      * reduce inflected or derived word into root
      * plurals, adverbs, etc
      * Bridge vocabulary gap
      * Risk: words of two meanings

    * Modern Search Engine  
      * No stemming or stopword removal?, computation resource less concerned
      * more advance NLP

## Lecture III: Inverted Index (data structure for crawled data)
  * Bag of words
    * For each document, a V sized (V is the whole vocabulary) boolean set to record each word's appearance
    * Assumption: words are independent from each other
      * Pros: simple
      * Cons
        * grammar and order are missing
        * Not space efficient O(D * V)
        * Time Complexity for Retrieval O(|q| * D * |D|)(|q| length of query, |D| length of a document)

        ```
        doclist = []
        for(wi in q) {
          for (d in D) { // unnecessary loop
            for (wj in d) { // unnecessary loop
              if (wi == wj) {
                doclist.append(d);
                break;
              }
            }
          }
        }
        ```
  * N-grams
    * contiguous sequence of n items from a given sequence
      * Pros: local dependency and order
      * increase vocabulary size

  * Zipf's law
    * Frequency of any word is inversely proportional to its rank in the frequency table
    * semantically meaning less word may take large portion of occurence
    * Tail words take major portion of the vocabulary but rarely in documents crawled
    * Middle ones most representative

  * Inverted index
    * only store used words is efficient
    * linked list based on vocabulary
    * Build look-up table for each word in vocabulary (words to doc)
    * Time Complexity O(|q| * |L|) (|L| average length of a word posting)
    * structure
      * Dictionary: modest size
        * need fast access
          * Stay in memory(hash, B-tree, trie)
      * Posting: huge
        * sequential access needed
        * stay in disc
        * contain lots of information
        * need compression
    * Sorting-based inverted index construction
      * sort by docId (Parse & Count)
        * in each doc parse out each words and its frequency
      * sort by termId (Local Sort)
        * in each document parsed, sort by termID
      * Merge sort based on document, finally in termID order
      * challenges
        * Document size exceeds memory
      * Key steps
        * sort by termID -> global merge sort(preserve docID)
      * Dynamic index update
        * periodically rebuild the index
        * Auxiliary index
          * keep index for new doc in memory
          * merge to index if size exceeds
      * Index compression
        * Benefits
          * save storage
          * increase cache efficiency
          * Improve disk-memory transfer rate?
        * coding theory
          * Huffman encoding(encode based on occurence)
        * Observations
          * Store gaps between IDs instead of doc ID
          * Zipf's law:
            * The more frequent the word the smaller the gaps
            * The less frequent the word, the shorter the list
          * biased distribution leads to compression
        * Solution:
          * Fewer bits to encode smaller integers

    * Query parsing
      * parse query syntax
      * perform same sequence as document
       * Tokenization -> Normalization -> stemming -> stopwords removal

    * Procedure
      * lookup query term in Dictionary
      * retrieve posting lists
      * Operation
        * AND: interect
        * OR: union
        * Not: diff
        * Scan through postings sequentially
      * Time Complexity O(|L1| + |L2|)
      * Phrase Query (e.g. computer science, not computer + science)
        * Generalized posting matching
        * plus condition check (e.g. T2.pos = T1.pos + 1)
          * Proximity Check  T2.pos - T1.pos <= k
        * Require term position stored in index
      * More and more info stored in index

    * Spelling Correction
      * Tolerate misspelled queries
      * solutions
        * choose nearest correct spelling
        * choose most common correct spelling
      * Proximity between query terms
        * edit distance
          * minimum edit needs to change to correct spelling
          * layout of keyboards
        * Context Proximity
          * enumerates possibilities
          * apply heuristics
        * Phonetic similarity
          * phonetic hashing (hash based on sound)

## Lecture IV: Classical Retrieval Evaluations
  * Information needs
    * reflected by user query
    * categories  
      * navigational
      * informational
      * transactional
  * satisfaction
    * Increased result clicks
    * repeated/increased visit
    * result relevance
  * Basic hypothesis
    * Retrieved documents relevance is a good proxy of system utility
  * Key elements for IR Evaluations
    * Documents collection
    * A test suite of information needs, by queries
    * Set of relevance judgment, e.g. relevant/nonrelevant for each documents
  * Seach relevance
    * Information needs translated into queries
    * relevance judged by information need instead of queries
  * Evalution metrics
    * As unranked retrieval Set
    * As ranked retrieval results
  * Unranked
    * Precision: p(relevant|retrieved) = TP/(TP + FP)
    * Recall: p(retrieved|relevant) = TP/(TP + FN)
    * Precision and recall trade off
      * precision decreases as number of docs increase, while recall keeps increasing
      * two metrics emphasize different perspective
       * Precision: prefer fewer docs, higher accuracy
       * Recall: prefer more documents retrieved
    * F measure: F = 1/(a/P + (1-a)/R)
      * either one metric is bad could drive F score down
  * Ranked retrieval results
    * precision, recall, F-measure are all set based, not Ranked
    * solution: evaluate **precision** at every **recall** point
      * Note: since relevant docs are fixed, more docs retrieved, recall increases, precision decreases
    * Binary relevance
      * Eleven-point interpolated average precision
        * At eleven recall points calculate the average of precision
      * Precision at K
        * set a rank position at K, compute precision in top K retrieved
        * also has recall at K
      * Mean Average Precision
        * For each relevant docs, retrieve their ranking K1, K2 ... Kn
        * compute P@K for each K
        * average precision = average P@K
        * MAP is mean of AP across multiple test queries
        * MAP metric
          * If a relevant doc never retrieved , AP = 0
          * MAP gets average for multiple queries
          * MAP assume user wants to get multiple relevant docs
          * MAP require lots of relevance judgment
      * Mean Reciprocal Rank
        * measure the effectiveness of the ranked results
        * suppose user only interested in one relevant doc
        * consider the rank of the first relevant doc
        * Reciprocal Rank = 1/K
        * MRR is the mean across multiple queries
    * ranked relevance
      * Discounted Cumulative gain
        * Uses graded relevance as a measure of usefulness
        * Typical discount is 1/log2(rank)
        * DCG is the total gain at a particular rank position
        * DCGp = sum(2^rel(i) - 1)/log(2)(1+i)
    * statistical significance tests
      * p-value is the probability of obtaining data we observed, if null hypothesis is true
      * if p-val smaller than a certain threshold, we can reject null hypothesis
      * seek to reject null hypothesis
      * sign test
      * Wilcoxon signed rank test
      * Paired t-test
    * Where to get ranked relevance
      * Domain experts
        * not usually consistent
        * but it doesn't really matter
      * measuring accessor consistency
        * Kappa statistic: K = (P(A) - P(E))/(1 - P(E))
          * P(A) is the proportion that they agree
          * P(E) is the proportion that they would agree by chance
          * K = 1 totally agree; K = 0 agree by chance; K < 0 doesn't agree
          * P(A) = (Yes/Yes + No/No)/total
          * P(E) = [(Yes/B + Yes/B)/(Total + Total)] ^ 2 + [(No/B + No/B)/(Total + Total)] ^ 2
      * Human annotation is time consuming and expensive
        * pooling
