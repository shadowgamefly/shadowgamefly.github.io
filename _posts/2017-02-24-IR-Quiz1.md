---
layout: post
title: 02/05/2017 Quiz I Notes
description: Quiz I Notes for Computer Graphics
modified: 2017-02-05
tags: [IR, notes]
---

## Lecture Zero: Intro

* Why IR
    * Due to information overload
    * Unstructured data

* Main topics
    * Search Engine Arch
    * Retrieval models
    * Evaluations
    * feedback
    * link analysis
    * Search applications

* Core Concepts
  * Query representation
    * lexical gap
    * Semantic gap
  * Document representation
    * data structure for access
    * handle lexical + semantic gap
  * Retrieval model
    * based on the query give **most relevant** result back

  * IR vs DB
  * IR vs NLP


## Lecture I: Search Engine Architecture

* Paper to read: **The Anatomy of a Large-Scale Hypertextual Web Search Engine**

* Key parts:
  * Crawler and indexer
  * Document Analyzer
  * Query parser
  * Ranking model

* Flow path:
  * query side:
    * user input -> Query parser -> ranking model -> domain db -> result preprocessing -> result display
  * document side:
    * crawler & indexer -> Document Analyzer & Auxiliary database

* Another looking:
  * crawler -> Doc Analyzer -> indexer ->
  * Query -> Query Rep ->
  * Combined to -> ranker -> result
  * Receive feedback -> modify ranker & Query Rep

* Key concepts
  * Information need
  * Query
  * Document
  * Relevance
  * **rank documents by their relevance to user's Information need**
  * Web crawler
  * Document Analyzer & indexer
      * manage crawled content and provide efficient access
  * Query parser
      * compile user input into comp. rep.
  * Ranking model
      * sort the candidate documents by its relevancy to the given query
  * Result Display
      * Present the retrieved results
  * Retrieval Evaluation
      * Assess the quality of the result
  * Relevance feedback
      * propagate the judgment back to the system to improve performance
  * Search Query log
      * record user's interaction with the system
  * User modeling
      * Assess user's satisfaction with the search engine
      * understand user longitude info need
  * Search Style
      * Browsing -- Browse all categories
      * Querying -- Completely based on user query
  * Pull vs Push
      * Pull, with query -- user with initiative pull info out
      * push, without query -- system push info out based on user log

## Lecture II: Web Crawling & Text analysis
  * Psuedo code

  ```
  Def Crawler(entry) {
    Stack = [entry]
    while (!Stack.empty)
      Current_URL = Stack.pop();
      if (visited(Current_URL)) | isIllegal(Current_URL) | checkRobotsText(Current_URL)
      continue
      page = open(Current_URL)
      for (anchor in page)
        Stack.append(anchor)
      setVisited(Current_URL)
      insertToIndex(HTML)
  }
  ```
  * Strategies
    * BFS
      * uniformly browse the web
      * like the pseudo code given
    * DFS
      * Explore the web by branch
      * web is not tree so biased ?
    * Focused Crawling
      * idea
        * prioritize links by given strategy
        * get a higher weighted average
      * prioritize by in-degree
        * Take the page with highest number of hyperlinks from previous web
      * PageRank
        * BFS in early stage then compute PageRank periodically
      * Topical relevance
        * only crawl relevant pages(vertical search)
        * Estimate similarity to current page(by anchortext, or something near)
        * user given taxonomy or topical classifier?
      * Avoid duplicate visit
        * use trie? or hash
        * most normalize url(truncate unnecessary tail) before check
      * Robot exclusion protocol

    * Text analysis
      * HTML parsing
        * shallow parsing
          * remove html tags
          * only take title/p
        * Automatic wrapper generation
          * wrapper generation
            * wrapper: regular expression for tags combination
            * inductive reasoning from examples
        * Visual parsing
          * Frequent pattern mining of similar html block
      * Tokenization
        * break stream of text into meaningful units (could be more than words)
        * Solutions
          * regular expression to parse whitespace and combine 's'
          * Statistical method to explore features of texts (NLP library)

    * Normalization
      * Convert different forms of words into the vocabulary
      * Solution
        * Rule-based: delete periods, lower cases, etc
        * Dictionary-based: construct equivalence class

    * Stemming
      * reduce inflected or derived word into root
      * plurals, adverbs, etc
      * Bridge vocabulary gap
      * Risk: words of two meanings

    * Modern Search Engine  
      * No stemming or stopword removal?, computation resource less concerned
      * more advance NLP


## Lecture III: Inverted Index (data structure for crawled data)
  * Bag of words
    * For each document, a V sized (V is the whole vocabulary) boolean set to record each word's appearance
    * Assumption: words are independent from each other
      * Pros: simple
      * Cons
        * grammar and order are missing
        * Not space efficient O(D * V)
        * Time Complexity for Retrieval O(|q| * D * |D|)(|q| length of query, |D| length of a document)

        ```
        doclist = []
        for(wi in q) {
          for (d in D) { // unnecessary loop
            for (wj in d) { // unnecessary loop
              if (wi == wj) {
                doclist.append(d);
                break;
              }
            }
          }
        }
        ```
  * N-grams
    * contiguous sequence of n items from a given sequence
      * Pros: local dependency and order
      * increase vocabulary size

  * Zipf's law
    * Frequency of any word is inversely proportional to its rank in the frequency table
    * semantically meaning less word may take large portion of occurence
    * Tail words take major portion of the vocabulary but rarely in documents crawled
    * Middle ones most representative

  * Inverted index
    * only store used words is efficient
    * linked list based on vocabulary
    * Build look-up table for each word in vocabulary (words to doc)
    * Time Complexity O(|q| * |L|) (|L| average length of a word posting)
    * structure
      * Dictionary: modest size
        * need fast access
          * Stay in memory(hash, B-tree, trie)
      * Posting: huge
        * sequential access needed
        * stay in disc
        * contain lots of information
        * need compression
        
